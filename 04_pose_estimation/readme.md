åŸºäºMediaPipeçš„äººä½“å§¿æ€ä¼°è®¡å®Œæ•´è§£å†³æ–¹æ¡ˆ,é€‚ç”¨äºäººå½¢æœºå™¨äººè§†è§‰æ„ŸçŸ¥ã€‚

## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install mediapipe opencv-python numpy
```

### åŸºç¡€ä½¿ç”¨

```bash
# å®æ—¶å§¿æ€æ£€æµ‹
python mediapipe_pose.py --source 0

# è·Œå€’æ£€æµ‹
python fall_detection.py --source 0

# æ‰‹åŠ¿è¯†åˆ«
python gesture_recognition.py --source 0

# æ·±è¹²è¯„ä¼°
python squat_evaluator.py --source 0
```

---

## ğŸ“‚ æ¨¡å—è¯´æ˜

| æ–‡ä»¶ | åŠŸèƒ½ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `mediapipe_pose.py` | åŸºç¡€å§¿æ€æ£€æµ‹ | é€šç”¨å§¿æ€ä¼°è®¡ |
| `fall_detection.py` | è·Œå€’æ£€æµ‹ | å®‰é˜²ç›‘æ§ã€è€äººçœ‹æŠ¤ |
| `gesture_recognition.py` | æ‰‹åŠ¿è¯†åˆ« | äººæœºäº¤äº’ã€ä½“æ„Ÿæ§åˆ¶ |
| `squat_evaluator.py` | è¿åŠ¨å§¿æ€è¯„ä¼° | å¥èº«åº”ç”¨ã€åŠ¨ä½œçº æ­£ |

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. åŸºç¡€å§¿æ€æ£€æµ‹

**è¾“å‡º:** 33ä¸ªäººä½“å…³é”®ç‚¹(x, y, z, visibility)

**å…³é”®ç‚¹ç´¢å¼•:**
```
0: é¼»å­
11-12: è‚©è†€
13-14: è‚˜
15-16: è…•
23-24: é«‹éƒ¨
25-26: è†ç›–
27-28: è¸
```

**ç¤ºä¾‹:**
```python
from mediapipe_pose import PoseDetector

detector = PoseDetector(model_complexity=1)
results, annotated = detector.detect(frame)

if results.pose_landmarks:
    landmarks = detector.get_all_landmarks(results)
    print(f"æ£€æµ‹åˆ° {len(landmarks)} ä¸ªå…³é”®ç‚¹")
```

---

### 2. è·Œå€’æ£€æµ‹

**åˆ¤å®šä¾æ®:**
- èº¯å¹²è§’åº¦ > 60Â° (æ¥è¿‘æ°´å¹³)
- é«‹éƒ¨é«˜åº¦ > 0.8 (æ¥è¿‘åœ°é¢)
- è¿ç»­Nå¸§æ»¡è¶³æ¡ä»¶

**å‚æ•°è°ƒæ•´:**
```python
detector = FallDetector(
    angle_threshold=60,      # èº¯å¹²è§’åº¦é˜ˆå€¼
    hip_threshold=0.8,       # é«‹éƒ¨é«˜åº¦é˜ˆå€¼
    confidence_window=15,    # æ—¶é—´çª—å£(å¸§)
    confidence_ratio=0.7     # ç½®ä¿¡åº¦æ¯”ä¾‹
)
```

**å…¸å‹è¾“å‡º:**
```
âœ“ æ­£å¸¸: angle=15Â°, hip=0.45
âœ— è·Œå€’: angle=75Â°, hip=0.85
```

---

### 3. æ‰‹åŠ¿è¯†åˆ«

**æ”¯æŒçš„æ‰‹åŠ¿:**
- æŒ¥æ‰‹ (wave)
- ä¸¾æ‰‹ (raise_hand)
- åŒæ‰‹ä¸¾èµ· (hands_up)
- æŒ‡å‘å·¦ä¾§/å³ä¾§ (point_left/right)
- åŒè‡‚äº¤å‰ (arms_crossed)

**è‡ªå®šä¹‰æ‰‹åŠ¿:**
```python
# åœ¨gesture_recognition.pyä¸­æ·»åŠ è§„åˆ™
def _match_gesture(self, features, landmarks):
    # ç¤ºä¾‹:æ£€æµ‹"OK"æ‰‹åŠ¿
    if thumb_and_index_form_circle():
        return 'ok', 0.9
```

---

### 4. è¿åŠ¨å§¿æ€è¯„ä¼°

**è¯„ä¼°æŒ‡æ ‡:**
- âœ… è†ç›–è§’åº¦ (70-100Â°)
- âœ… èƒŒéƒ¨è§’åº¦ (< 20Â°)
- âœ… è†ç›–ä¸è¶…è„šå°–
- âœ… è†ç›–ä¸å†…æ‰£

**è¯„åˆ†ç³»ç»Ÿ:**
```
100åˆ†: å®Œç¾å§¿æ€
90+åˆ†: ä¼˜ç§€
70+åˆ†: è‰¯å¥½
<70åˆ†: éœ€æ”¹è¿›
```

**å®æ—¶è®¡æ•°:**
- è‡ªåŠ¨è¯†åˆ«æ·±è¹²çš„"èµ·-è¹²-èµ·"å¾ªç¯
- åªåœ¨å®Œæ•´åŠ¨ä½œåè®¡æ•°

---

## âš™ï¸ å‚æ•°é…ç½®

### æ¨¡å‹å¤æ‚åº¦

```python
model_complexity=0  # Lite:  å¿«é€Ÿä½†ç²¾åº¦è¾ƒä½
model_complexity=1  # Full:  å¹³è¡¡æ¨¡å¼(æ¨è)
model_complexity=2  # Heavy: é«˜ç²¾åº¦ä½†è¾ƒæ…¢
```

**æ€§èƒ½å¯¹æ¯”(640Ã—480):**

| æ¨¡å‹ | Jetson Nano | Jetson Xavier NX | æ¡Œé¢CPU |
|------|-------------|------------------|---------|
| Lite | 25ms (40 FPS) | 12ms (83 FPS) | 15ms (66 FPS) |
| Full | 40ms (25 FPS) | 20ms (50 FPS) | 25ms (40 FPS) |
| Heavy | 70ms (14 FPS) | 38ms (26 FPS) | 45ms (22 FPS) |

### ç½®ä¿¡åº¦é˜ˆå€¼

```python
min_detection_confidence=0.5  # æ£€æµ‹é˜ˆå€¼(é¦–æ¬¡æ£€æµ‹)
min_tracking_confidence=0.5   # è·Ÿè¸ªé˜ˆå€¼(åç»­å¸§)
```

**å»ºè®®è®¾ç½®:**
- å…‰çº¿å¥½ã€å§¿æ€æ¸…æ™°: 0.7
- ä¸€èˆ¬æƒ…å†µ: 0.5 (é»˜è®¤)
- é®æŒ¡è¾ƒå¤šã€å…‰çº¿å·®: 0.3

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. é™ä½åˆ†è¾¨ç‡

```python
# ä»1080pé™åˆ°480p
frame = cv2.resize(frame, (640, 480))
results, annotated = detector.detect(frame)

# é€Ÿåº¦æå‡: 2-3x
# ç²¾åº¦æŸå¤±: <5%
```

### 2. è·³å¸§å¤„ç†

```python
frame_count = 0
process_every = 2  # æ¯2å¸§å¤„ç†ä¸€æ¬¡

while True:
    ret, frame = cap.read()
    frame_count += 1
    
    if frame_count % process_every == 0:
        results, annotated = detector.detect(frame)
    
    # ä½¿ç”¨ä¸Šæ¬¡ç»“æœæ˜¾ç¤º
    cv2.imshow('Pose', annotated)
```

### 3. å¤šçº¿ç¨‹(é«˜çº§)

```python
from threading import Thread
from queue import Queue

# åˆ†ç¦»æ•è·å’Œå¤„ç†
capture_thread = Thread(target=capture_frames)
process_thread = Thread(target=process_poses)

capture_thread.start()
process_thread.start()
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: æ£€æµ‹ä¸åˆ°äººæˆ–å…³é”®ç‚¹æŠ–åŠ¨

**è§£å†³:**
```python
# 1. æé«˜ç½®ä¿¡åº¦
detector = PoseDetector(min_detection_confidence=0.7)

# 2. æ”¹å–„å…‰çº¿
# - ç¡®ä¿æ­£é¢å…‰ç…§
# - é¿å…é€†å…‰

# 3. æ—¶é—´å¹³æ»‘
# - ä½¿ç”¨ç§»åŠ¨å¹³å‡æ»¤æ³¢å…³é”®ç‚¹
```

### Q2: Jetsonä¸Šè¿è¡Œæ…¢

**ä¼˜åŒ–ç­–ç•¥:**
```python
# 1. ä½¿ç”¨Liteæ¨¡å‹
model_complexity=0

# 2. é™ä½åˆ†è¾¨ç‡
frame = cv2.resize(frame, (320, 240))

# 3. è·³å¸§
process_every_n_frames = 2
```

### Q3: ä¾§é¢/èƒŒé¢æ£€æµ‹å¤±è´¥

**åŸå› :** MediaPipeä¸»è¦é’ˆå¯¹æ­£é¢/æ–œä¾§é¢è®­ç»ƒ

**è§£å†³:**
```python
# æ£€æŸ¥å¯è§æ€§
def check_visibility(landmarks):
    left_visible = landmarks[11].visibility > 0.5  # å·¦è‚©
    right_visible = landmarks[12].visibility > 0.5  # å³è‚©
    
    if not (left_visible or right_visible):
        return "èƒŒé¢æˆ–è¢«é®æŒ¡"
```

### Q4: å¤šäººåœºæ™¯åªæ£€æµ‹ä¸€ä¸ª

**è§£å†³:** å…ˆç”¨YOLOæ£€æµ‹æ‰€æœ‰äºº,å†é€ä¸ªå§¿æ€ä¼°è®¡

```python
# ä¼ªä»£ç 
people_boxes = yolo.detect(frame)

for box in people_boxes:
    person_roi = crop(frame, box)
    pose_result = mediapipe_pose.process(person_roi)
```

---

## ğŸ“Š å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: å¥èº«è®¡æ•°å™¨

```python
class FitnessCounter:
    def __init__(self, exercise_type='squat'):
        self.count = 0
        self.state = 'up'
    
    def update(self, knee_angle):
        if self.state == 'up' and knee_angle < 100:
            self.state = 'down'
        elif self.state == 'down' and knee_angle > 140:
            self.state = 'up'
            self.count += 1
        
        return self.count
```

### æ¡ˆä¾‹2: è™šæ‹Ÿè¯•è¡£

```python
# åŸºäºè‚©å®½å’Œèº«é«˜ä¼°ç®—è¡£æœå°ºå¯¸
shoulder_width = distance(left_shoulder, right_shoulder)
body_height = distance(nose, ankle)

if shoulder_width < 0.2 and body_height < 0.7:
    size = 'S'
elif shoulder_width < 0.25 and body_height < 0.8:
    size = 'M'
else:
    size = 'L'
```

### æ¡ˆä¾‹3: å§¿æ€çŸ«æ­£æé†’

```python
if back_angle > 30:
    alert("è¯·æŒºç›´èƒŒéƒ¨!")

if knee_over_toe:
    alert("è†ç›–ä¸è¦è¶…è¿‡è„šå°–!")
```

---

## ğŸ“ˆ 33ä¸ªå…³é”®ç‚¹è¯¦ç»†è¯´æ˜

```
è„¸éƒ¨ (0-10):
  0: nose (é¼»å­)
  1-2: left/right eye inner (å†…çœ¼è§’)
  3-4: left/right eye (çœ¼ç›)
  5-6: left/right eye outer (å¤–çœ¼è§’)
  7-8: left/right ear (è€³æœµ)
  9-10: mouth left/right (å˜´è§’)

ä¸Šè‚¢ (11-22):
  11-12: left/right shoulder (è‚©è†€)
  13-14: left/right elbow (è‚˜)
  15-16: left/right wrist (è…•)
  17-18: left/right pinky (å°æŒ‡)
  19-20: left/right index (é£ŸæŒ‡)
  21-22: left/right thumb (æ‹‡æŒ‡)

ä¸‹è‚¢ (23-32):
  23-24: left/right hip (é«‹)
  25-26: left/right knee (è†)
  27-28: left/right ankle (è¸)
  29-30: left/right heel (è„šè·Ÿ)
  31-32: left/right foot index (è„šå°–)
```

---

## ğŸ“ è¿›é˜¶å­¦ä¹ 

### 3Dåæ ‡ç³»ç»Ÿ

```python
# zåæ ‡æ˜¯ç›¸å¯¹æ·±åº¦(ç›¸å¯¹äºé«‹éƒ¨ä¸­å¿ƒ)
landmark = results.pose_landmarks.landmark[15]  # å³æ‰‹è…•

x = landmark.x  # å½’ä¸€åŒ–åæ ‡ 0-1
y = landmark.y  # å½’ä¸€åŒ–åæ ‡ 0-1
z = landmark.z  # ç›¸å¯¹æ·±åº¦(ç±³)
visibility = landmark.visibility  # å¯è§æ€§ 0-1

# z < 0: åœ¨èº«ä½“å‰æ–¹
# z > 0: åœ¨èº«ä½“åæ–¹
```

### åŠ¨ä½œè¯†åˆ«(æ—¶åº)

```python
# æ”¶é›†30å¸§ä½œä¸ºä¸€ä¸ªåŠ¨ä½œåºåˆ—
sequence = []
for frame in video[:30]:
    landmarks = extract_landmarks(frame)
    sequence.append(landmarks)

# è¾“å…¥LSTMæ¨¡å‹åˆ†ç±»
action = lstm_model.predict(sequence)
# è¾“å‡º: "walking", "running", "jumping", etc.
```

---

## ğŸ“ å‘½ä»¤è¡Œå‚æ•°

### mediapipe_pose.py

```bash
python mediapipe_pose.py \
    --source 0                # è¾“å…¥æº(æ‘„åƒå¤´/è§†é¢‘)
    --model 1                 # æ¨¡å‹å¤æ‚åº¦(0/1/2)
    --confidence 0.5          # æ£€æµ‹ç½®ä¿¡åº¦
    --output output.mp4       # ä¿å­˜è§†é¢‘
```

### fall_detection.py

```bash
python fall_detection.py \
    --source 0                    # è¾“å…¥æº
    --angle-threshold 60          # è§’åº¦é˜ˆå€¼
    --hip-threshold 0.8           # é«‹éƒ¨é˜ˆå€¼
    --alarm-sound alarm.wav       # æŠ¥è­¦å£°éŸ³
```

### gesture_recognition.py

```bash
python gesture_recognition.py \
    --source 0                # è¾“å…¥æº
```

### squat_evaluator.py

```bash
python squat_evaluator.py \
    --source 0                # è¾“å…¥æº
```

---

## ğŸ”— ç›¸å…³èµ„æº

**MediaPipeå®˜æ–¹:**
- æ–‡æ¡£: https://google.github.io/mediapipe/solutions/pose
- GitHub: https://github.com/google/mediapipe

**è®ºæ–‡:**
- BlazePose: On-device Real-time Body Pose tracking (2020)

**é…å¥—æ–‡ç« :**
- çŸ¥ä¹: ã€Šäººå½¢æœºå™¨äººè§†è§‰(å››):MediaPipeäººä½“å§¿æ€ä¼°è®¡å®æˆ˜ã€‹

---

## ğŸ“„ License

MIT License

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPR!

**å¼€å‘è®¡åˆ’:**
- [ ] å¤šäººå§¿æ€æ£€æµ‹
- [ ] æ›´å¤šè¿åŠ¨ç±»å‹è¯„ä¼°(ä¿¯å§æ’‘ã€å¼•ä½“å‘ä¸Š)
- [ ] åŠ¨ä½œè¯†åˆ«LSTMæ¨¡å‹
- [ ] ROS2é›†æˆèŠ‚ç‚¹

---

## âœ¨ å¿«é€Ÿæµ‹è¯•

```bash
# 1. å®‰è£…ä¾èµ–
pip install mediapipe opencv-python numpy

# 2. è¿è¡ŒåŸºç¡€æ£€æµ‹
python mediapipe_pose.py --source 0

# 3. å°è¯•è·Œå€’æ£€æµ‹(å‡è£…æ‘”å€’)
python fall_detection.py --source 0

# 4. å°è¯•æ‰‹åŠ¿è¯†åˆ«(æŒ¥æ‰‹ã€ä¸¾æ‰‹)
python gesture_recognition.py --source 0

# 5. åšå‡ ä¸ªæ·±è¹²
python squat_evaluator.py --source 0
```

---

**æœ€åæ›´æ–°:** 2025å¹´12æœˆ

**ç‰ˆæœ¬:** v1.0.0

